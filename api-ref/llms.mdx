---
title: "Build with LLMs"
sidebarTitle: "LLMs"
icon: "computer"
---


# Building with Large Language Models (LLMs)

You can use large language models (LLMs) to assist in the building of Invopop integrations. We provide a set of tools and best practices if you use LLMs during development.

## Plain Text Docs

You can access all of our documentation as plain text markdown files by adding `.md` to the end of any url. For example, you can find the plain text version of this page itself at [https://docs.invopop.com/building-with-llms.md](https://docs.invopop.com/building-with-llms.md).

This helps AI tools and agents consume our content and allows you to copy and paste the entire contents of a doc into an LLM. This format is preferable to scraping or copying from our HTML and JavaScript-rendered pages because:

- Plain text contains fewer formatting tokens.
- Content that isn't rendered in the default view (for example, it's hidden in a tab) of a given page is rendered in the plain text version.
- LLMs can parse and understand markdown hierarchy.

We also host an [`/llms.txt` file](https://docs.invopop.com/llms.txt) which instructs AI tools and agents how to retrieve the plain text versions of our pages. The `/llms.txt` file is an [emerging standard](https://llmstxt.org/) for making websites and content more accessible to LLMs.

## invopop Model Context Protocol (MCP) Server

For developers using code editors that use AI, such as Cursor or Windsurf, or general purpose tools such as Claude Desktop, we provide the [invopop Model Context Protocol (MCP)](https://github.com/invopop/agent-toolkit/tree/main/modelcontextprotocol) server. The MCP server provides AI agents a set of tools for calling the invopop API and searching our knowledge base (documentation, support articles, and so on).

### CLI

Run the following command to start the MCP server locally:

```bash
npx -y @invopop/mcp --tools=all --api-key=sk_test_51MSUkPHlPoaS9ika8DQUE3C74G1u2vRJhDCWMTKiltMlJ7nGqXLXMXZhOSW1FcRg3sj7BWWN7sOylPvol5SVJHT900WnVfvt5l
```

### Cursor

Add the following to your `.cursor/mcp.json` file. Go to **Cursor Settings** > **MCP** and make sure the server is enabled and the selected tools are available. See the [Cursor documentation](https://docs.cursor.com/context/model-context-protocol) for more details.

```json
{
  "mcpServers": {
    "invopop": {
      "command": "npx",
      "args": [
          "-y",
          "@invopop/mcp",
          "--tools=all",
          "--api-key=sk_test_51MSUkPHlPoaS9ika8DQUE3C74G1u2vRJhDCWMTKiltMlJ7nGqXLXMXZhOSW1FcRg3sj7BWWN7sOylPvol5SVJHT900WnVfvt5l"
      ]
    }
  }
}
```

### Claude Desktop

Add the following to your `claude_desktop_config.json` file. See the [Claude Desktop documentation](https://modelcontextprotocol.io/quickstart/user) for more details.

```json
{
  "mcpServers": {
    "invopop": {
      "command": "npx",
      "args": [
          "-y",
          "@invopop/mcp",
          "--tools=all",
          "--api-key=sk_test_51MSUkPHlPoaS9ika8DQUE3C74G1u2vRJhDCWMTKiltMlJ7nGqXLXMXZhOSW1FcRg3sj7BWWN7sOylPvol5SVJHT900WnVfvt5l"
      ]
    }
  }
}
```

The code editor agent automatically detects all the available tools and calls the relevant tool when you post a related question in the chat.

## VS Code AI Assistant

If you're a Visual Studio Code user, you can install the [invopop VS Code extension](https://docs.invopop.com/invopop-vscode) to access our AI Assistant.

With the invopop AI Assistant, you can:

- Get immediate answers about the invopop API and products
- Receive code suggestions tailored to your integration
- Ask follow-up questions for more detailed information
- Access knowledge from the invopop documentation and the invopop developer community

To get started with the invopop AI assistant:

1. Make sure you have the invopop VS Code extension installed.
2. Navigate to the invopop extension UI
3. Under **AI Assistant** click **Ask a question**.
   - If you're a Copilot user, this opens the Copilot chat where you can @-mention `@invopop`. In the input field, talk to the invopop-specific assistant using `@invopop` followed by your question.
   - If you're not a Copilot user, it opens a chat UI where you can talk to the invopop LLM directly.

## invopop Agent Toolkit SDK

If you're building agentic software, we provide an SDK for adding invopop functionality to your agent's capabilities. For example, using the SDK you can:

- Create invopop objects
- Charge for agent usage
- Use with popular frameworks such as OpenAI's Agent SDK, Vercel's AI SDK, Langchain, and CrewAI

Learn more in our [agents documentation](https://docs.invopop.com/agents).

## See also

- [invopop for Visual Studio Code](https://docs.invopop.com/invopop-vscode)
- [Add invopop to your agentic workflows](https://docs.invopop.com/agents)